{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Declare imports and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "INPUT_DIR = \"input\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "VEDDRA_FILE = os.path.join(OUTPUT_DIR, 'veddra.csv')\n",
    "\n",
    "PRODUCTS = ['librela', 'solensia']\n",
    "INPUT_DIR_EVVET = os.path.join(INPUT_DIR, 'evvet')\n",
    "INPUT_DIR_EVVET_PRODUCTS = [os.path.join(INPUT_DIR_EVVET, product) for product in PRODUCTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create veddra lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Eye disorders', 'Iris, ciliary body and choroid disorders')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veddra = pd.read_csv(VEDDRA_FILE)\n",
    "\n",
    "# Create a dictionary for O(1) lookup\n",
    "veddra_lookup = {}\n",
    "\n",
    "# Populate the dictionary with PT and LLT as keys\n",
    "for _, row in veddra.iterrows():\n",
    "    pt = row['Current Preferred Term (PT)']\n",
    "    llt = row['Current Low Level Term (LLT)']\n",
    "    soc = row['Current System Organ Class (SOC) Term']\n",
    "    hlt = row['Current High Level Term (HLT)']\n",
    "    \n",
    "    # Add PT to the dictionary\n",
    "    veddra_lookup[pt] = (soc, hlt)\n",
    "    \n",
    "    # Add LLT to the dictionary\n",
    "    veddra_lookup[llt] = (soc, hlt)\n",
    "\n",
    "display(veddra_lookup['Uveitis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate all output files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Processing Librela EVVet data...\n",
      "===========================================\n",
      "\n",
      "Last run:\n",
      "February 06, 2025 20:58:57 PST\n",
      "\n",
      "Processing file: librela-2025.csv...\n",
      "Number of rows: 1562\n",
      "\n",
      "Processing file: librela-2024.csv...\n",
      "Number of rows: 15289\n",
      "\n",
      "Processing file: librela-2023.csv...\n",
      "Number of rows: 5105\n",
      "\n",
      "Processing file: librela-2022.csv...\n",
      "Number of rows: 3169\n",
      "\n",
      "Processing file: librela-2021.csv...\n",
      "Number of rows: 383\n",
      "\n",
      "VeDDRA SOC and HLT columns added to the dataframe...\n",
      "\n",
      "New data found. Proceeding with the update...\n",
      "\n",
      "Master file compiled and written to archive and source.\n",
      "\n",
      "Total cases: 25508\n",
      "Animals affected: 26617\n",
      "Animals treated: 30485.0\n",
      "Animals died: 3335.0\n",
      "\n",
      "output/evvet/librela/librela_meta.json updated!\n",
      "\n",
      "===========================================\n",
      "Processing Solensia EVVet data...\n",
      "===========================================\n",
      "\n",
      "Last run:\n",
      "February 06, 2025 20:58:58 PST\n",
      "\n",
      "Processing file: solensia-2023.csv...\n",
      "Number of rows: 4538\n",
      "\n",
      "Processing file: solensia-2022.csv...\n",
      "Number of rows: 1696\n",
      "\n",
      "Processing file: solensia-2021.csv...\n",
      "Number of rows: 67\n",
      "\n",
      "Processing file: solensia-2025.csv...\n",
      "Number of rows: 397\n",
      "\n",
      "Processing file: solensia-2024.csv...\n",
      "Number of rows: 5002\n",
      "\n",
      "VeDDRA SOC and HLT columns added to the dataframe...\n",
      "\n",
      "New data found. Proceeding with the update...\n",
      "\n",
      "Master file compiled and written to archive and source.\n",
      "\n",
      "Total cases: 11700\n",
      "Animals affected: 12102\n",
      "Animals treated: 12575.0\n",
      "Animals died: 977.0\n",
      "\n",
      "output/evvet/solensia/solensia_meta.json updated!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "def generate(PRODUCT):\n",
    "    INPUT_DIR_EVVET = os.path.join(INPUT_DIR, 'evvet', PRODUCT)\n",
    "    OUTPUT_DIR_EVVET = os.path.join(OUTPUT_DIR, 'evvet', PRODUCT)\n",
    "    OUTPUT_EVVET_FILE = os.path.join(OUTPUT_DIR_EVVET, f\"{PRODUCT}.csv\")\n",
    "    OUTPUT_EVVET_META = os.path.join(OUTPUT_DIR_EVVET, f\"{PRODUCT}_meta.json\")\n",
    "\n",
    "    # Define the PDT timezone\n",
    "    pdt = pytz.timezone('America/Los_Angeles')\n",
    "    \n",
    "    print()\n",
    "    print(f\"===========================================\")\n",
    "    print(f\"Processing {PRODUCT.capitalize()} EVVet data...\")\n",
    "    print(f\"===========================================\")\n",
    "    print()\n",
    "\n",
    "    # Get the current date and time in UTC and convert to PDT\n",
    "    current_datetime = datetime.datetime.now(pytz.utc).astimezone(pdt)\n",
    "    last_updated = current_datetime.strftime(\"%B %d, %Y %H:%M:%S %Z\")\n",
    "    print(f\"Last run:\\n{last_updated}\")\n",
    "\n",
    "    # # Delete the output file if it exists\n",
    "    # if os.path.exists(OUTPUT_EVVET_FILE):\n",
    "    #     os.remove(OUTPUT_EVVET_FILE)\n",
    "\n",
    "    # List all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(INPUT_DIR_EVVET) if f.endswith('.csv')]\n",
    "    \n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(INPUT_DIR_EVVET, file))\n",
    "        df.dropna(how='all', inplace=True)\n",
    "        print(f\"\\nProcessing file: {file}...\")\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes into one\n",
    "    master_df = pd.concat(df_list, ignore_index=True)\n",
    "    master_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Reorder columns to move 'AER form' to the last index if it exists in the dataframe\n",
    "    if 'AER form' in master_df.columns:\n",
    "        cols = master_df.columns.tolist()\n",
    "        cols.append(cols.pop(cols.index('AER form')))\n",
    "        master_df = master_df[cols]\n",
    "\n",
    "    # Move 'Drug' column to the end if it exists in the dataframe\n",
    "    if 'Drug' in master_df.columns:\n",
    "        cols = master_df.columns.tolist()\n",
    "        cols.append(cols.pop(cols.index('Drug')))\n",
    "        master_df = master_df[cols]\n",
    "\n",
    "    # Move 'Received date' column to the first index if it exists in the dataframe\n",
    "    if 'Received date' in master_df.columns:\n",
    "        cols = master_df.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('Received date')))\n",
    "        master_df = master_df[cols]\n",
    "\n",
    "    # Change Received Date column's format\n",
    "    master_df['Received date'] = pd.to_datetime(master_df['Received date'])\n",
    "\n",
    "    # Set the index to 'Received date' for master_df\n",
    "    master_df.set_index('Received date', inplace=True)\n",
    "\n",
    "    # Sort the dataframe by 'Received date'\n",
    "    master_df.sort_values(by='Received date', inplace=True)\n",
    "\n",
    "    # Drop any empty rows in master_df\n",
    "    master_df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Check for any rows where dates couldn't be parsed\n",
    "    if master_df.index.isna().any():\n",
    "        print(\"Some dates couldn't be parsed and were set to NaT\")\n",
    "\n",
    "    #####\n",
    "    # Add VeDDRA SOC and HLT columns to the evvet DataFrame\n",
    "    #####\n",
    "    # Initialize the columns\n",
    "    master_df['VeDDRA SOC'] = ''\n",
    "    master_df['VeDDRA HLT'] = ''\n",
    "\n",
    "    # Define a function to process each row\n",
    "    def process_row(row):\n",
    "        reactions = row['Reaction'].split(',')\n",
    "        soc_matches = set()\n",
    "        hlt_matches = set()\n",
    "        for reaction in reactions:\n",
    "            reaction = reaction.strip()\n",
    "            if reaction in veddra_lookup:\n",
    "                soc, hlt = veddra_lookup[reaction]\n",
    "                soc_matches.add(soc)\n",
    "                hlt_matches.add(hlt)\n",
    "        \n",
    "        row['VeDDRA SOC'] = '; '.join(sorted(soc_matches)) if soc_matches else ''\n",
    "        row['VeDDRA HLT'] = '; '.join(sorted(hlt_matches)) if hlt_matches else ''\n",
    "        return row\n",
    "\n",
    "    # Apply the function to each row\n",
    "    master_df = master_df.apply(process_row, axis=1)\n",
    "\n",
    "    # Remove the columns from their current positions\n",
    "    veddra_soc = master_df.pop('VeDDRA SOC')\n",
    "    veddra_hlt = master_df.pop('VeDDRA HLT')\n",
    "    # Insert the columns at the desired positions\n",
    "    master_df.insert(11, 'VeDDRA SOC', veddra_soc)\n",
    "    master_df.insert(12, 'VeDDRA HLT', veddra_hlt)\n",
    "\n",
    "    print()\n",
    "    print(\"VeDDRA SOC and HLT columns added to the dataframe...\")\n",
    "\n",
    "    # Load the most recent CSV listed in the meta file\n",
    "    with open(OUTPUT_EVVET_META, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    if meta['csvs']:\n",
    "        last_csv_info = meta['csvs'][0]\n",
    "        last_master_df = pd.read_csv(os.path.join(OUTPUT_DIR_EVVET, last_csv_info['name']), index_col='Received date', parse_dates=True)\n",
    "        \n",
    "\n",
    "        # Fill NaN values with an empty string to ensure that \"NaN\" and empty don't trigger a difference\n",
    "        master_df[['VeDDRA SOC', 'VeDDRA HLT']] = master_df[['VeDDRA SOC', 'VeDDRA HLT']].fillna('')\n",
    "        last_master_df[['VeDDRA SOC', 'VeDDRA HLT']] = last_master_df[['VeDDRA SOC', 'VeDDRA HLT']].fillna('')\n",
    "\n",
    "        # Check if the dataframes are identical\n",
    "        if master_df.equals(last_master_df):\n",
    "            print()\n",
    "            print(\"Duplicate fetch. Aborted!\")\n",
    "            return last_master_df\n",
    "        else: \n",
    "            print()\n",
    "            print(\"New data found. Proceeding with the update...\")\n",
    "            \n",
    "    # Write the updated master dataframe to a new CSV\n",
    "    new_csv_name = f\"{PRODUCT}_{current_datetime.strftime('%Y%m%d')}.csv\"\n",
    "    # Write to csv archive\n",
    "    master_df.to_csv(os.path.join(OUTPUT_DIR_EVVET, new_csv_name))\n",
    "    # Make this the new master file (copy)\n",
    "    master_df.to_csv(OUTPUT_EVVET_FILE)\n",
    "    \n",
    "    print(\"\\nMaster file compiled and written to archive and source.\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Total cases: {len(master_df)}\")\n",
    "    print(f\"Animals affected: {master_df['Animals affected'].sum()}\")\n",
    "    print(f\"Animals treated: {master_df['Animals treated'].sum()}\")\n",
    "    print(f\"Animals died: {master_df['Animals died'].sum()}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Update the meta file\n",
    "    new_csv_info = {\n",
    "        \"id\": last_csv_info['id'] + 1 if meta['csvs'] else 1,\n",
    "        \"name\": new_csv_name,\n",
    "        \"timestamp\": current_datetime.isoformat()\n",
    "    }\n",
    "    meta['csvs'].insert(0, new_csv_info)\n",
    "    \n",
    "    with open(OUTPUT_EVVET_META, \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)\n",
    "    print(f'{OUTPUT_EVVET_META} updated!')\n",
    "\n",
    "    return master_df\n",
    "\n",
    "evvet = {}\n",
    "\n",
    "for PRODUCT in PRODUCTS:\n",
    "    evvet[PRODUCT] = generate(PRODUCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Proportional Librela back-distribution of Feb 2022 Reporting Delay (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = evvet.copy()\n",
    "    # data.set_index('Received date', inplace=True)\n",
    "    # data['Received date'] = pd.to_datetime(data['Received date'])\n",
    "\n",
    "    # Group data and reset index to handle aggregation properly\n",
    "    aggregated_data = data.groupby('Received date').agg(\n",
    "        Animals_affected=('Animals affected', 'sum'),\n",
    "        Animals_died=('Animals died', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Rename columns to remove underscores\n",
    "    aggregated_data.rename(columns={'Animals_affected': 'Animals affected', 'Animals_died': 'Animals died'}, inplace=True)\n",
    "\n",
    "    # Now set 'Received date' as index for date-based operations\n",
    "    aggregated_data.set_index('Received date', inplace=True)\n",
    "\n",
    "    # Store original data for verification and retain original columns\n",
    "    original_totals = {\n",
    "        'rows': len(aggregated_data),\n",
    "        'Animals affected': aggregated_data['Animals affected'].sum(),\n",
    "        'Animals died': aggregated_data['Animals died'].sum()\n",
    "    }\n",
    "    aggregated_data['Animals affected original'] = aggregated_data['Animals affected']\n",
    "    aggregated_data['Animals died original'] = aggregated_data['Animals died']\n",
    "\n",
    "    start_date = pd.to_datetime('2021-02-08')\n",
    "    # Define outlier dates and their respective redistribution periods\n",
    "    outlier_dates = pd.to_datetime(['2022-02-10', '2022-02-11', '2022-07-21', '2023-01-19', '2024-05-06'])\n",
    "    redistribution_periods = {\n",
    "        outlier_dates[0]: (start_date, outlier_dates[0] - pd.Timedelta(days=1)),\n",
    "        outlier_dates[1]: (start_date, outlier_dates[1] - pd.Timedelta(days=1)),\n",
    "        outlier_dates[2]: (outlier_dates[1] + pd.Timedelta(days=1), outlier_dates[2] - pd.Timedelta(days=1)),\n",
    "        outlier_dates[3]: (outlier_dates[2] + pd.Timedelta(days=1), outlier_dates[3] - pd.Timedelta(days=1)),\n",
    "        outlier_dates[4]: (outlier_dates[4] - pd.DateOffset(months=6), outlier_dates[4] - pd.Timedelta(days=1))\n",
    "    }\n",
    "\n",
    "    def redistribute_cases(aggregated_data, outlier_dates, redistribution_periods, threshold=0.95):\n",
    "        for index, outlier_date in enumerate(outlier_dates):\n",
    "            period = redistribution_periods[outlier_date]\n",
    "            start_date, end_date = period\n",
    "            period_data = aggregated_data[(aggregated_data.index >= start_date) & (aggregated_data.index <= end_date)]\n",
    "            \n",
    "            # Calculate the threshold for the period\n",
    "            threshold_affected = int(period_data['Animals affected'].quantile(threshold))\n",
    "            threshold_died = int(period_data['Animals died'].quantile(threshold))\n",
    "            \n",
    "            # Identify excess cases\n",
    "            excess_affected = int(aggregated_data.loc[outlier_date, 'Animals affected']) - threshold_affected\n",
    "            excess_died = int(aggregated_data.loc[outlier_date, 'Animals died']) - threshold_died\n",
    "            \n",
    "            if excess_affected > 0:\n",
    "                redistribute_column_cases(aggregated_data, period, outlier_date, excess_affected, 'Animals affected')\n",
    "            if excess_died > 0:\n",
    "                redistribute_column_cases(aggregated_data, period, outlier_date, excess_died, 'Animals died')\n",
    "        \n",
    "        return aggregated_data\n",
    "\n",
    "    def redistribute_column_cases(aggregated_data, period, outlier_date, excess_cases, column):\n",
    "        start_date, end_date = period\n",
    "        redistribution_data = aggregated_data[(aggregated_data.index >= start_date) & (aggregated_data.index <= end_date)].copy()\n",
    "        \n",
    "        # Calculate the proportional distribution\n",
    "        total_past_cases = redistribution_data[column].sum()\n",
    "        if total_past_cases == 0:\n",
    "            return  # Skip if there are no past cases to redistribute into\n",
    "\n",
    "        redistribution_data.loc[:, 'proportion'] = redistribution_data[column] / total_past_cases\n",
    "        \n",
    "        # Redistribute cases proportionally\n",
    "        cases_distributed = 0\n",
    "        for index, row in redistribution_data.iterrows():\n",
    "            cases_to_add = int(row['proportion'] * excess_cases)\n",
    "            aggregated_data.loc[index, column] += cases_to_add\n",
    "            cases_distributed += cases_to_add\n",
    "        \n",
    "        # Handle remaining cases\n",
    "        remaining_cases = excess_cases - cases_distributed\n",
    "        for index in redistribution_data.index:\n",
    "            if remaining_cases <= 0:\n",
    "                break\n",
    "            aggregated_data.loc[index, column] += 1\n",
    "            remaining_cases -= 1\n",
    "\n",
    "        # Adjust the outlier date count\n",
    "        aggregated_data.loc[outlier_date, column] -= excess_cases\n",
    "\n",
    "    # Perform the redistribution\n",
    "    aggregated_data = redistribute_cases(aggregated_data, outlier_dates, redistribution_periods)\n",
    "\n",
    "    # Verify final totals\n",
    "    affected_sum = aggregated_data['Animals affected'].sum()\n",
    "    died_sum = aggregated_data['Animals died'].sum()\n",
    "    row_count = len(aggregated_data)\n",
    "\n",
    "    assert affected_sum == original_totals['Animals affected'], f\"Mismatch in Animals affected totals: expected {original_totals['Animals affected']}, got {affected_sum}\"\n",
    "    assert died_sum == original_totals['Animals died'], f\"Mismatch in Animals died totals: expected {original_totals['Animals died']}, got {died_sum}\"\n",
    "    assert row_count == original_totals['rows'], f\"Row count mismatch: expected {original_totals['rows']}, got {row_count}\"\n",
    "\n",
    "    print()\n",
    "    print(\"Redistribution (beta) complete!\")\n",
    "    print(f\"Total cases: {len(aggregated_data)}\")\n",
    "    print(f\"Animals affected: {aggregated_data['Animals affected'].sum()}\")\n",
    "    print(f\"Animals died: {aggregated_data['Animals died'].sum()}\")\n",
    "\n",
    "    # Drop the original columns before displaying the data\n",
    "    aggregated_data.drop(columns=['Animals affected original', 'Animals died original'], inplace=True)\n",
    "\n",
    "    # Output the modified dataset\n",
    "    aggregated_data.to_csv(OUTPUT_EVVET_ADJ)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "librela",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
